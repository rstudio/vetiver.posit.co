[
  {
    "objectID": "get-started/monitor.html",
    "href": "get-started/monitor.html",
    "title": "Monitor",
    "section": "",
    "text": "Once a model is deployed, it is important to monitor its statistical performance. Machine learning can break quietly; a model can continue returning predictions without error, even if it is performing poorly. Often these quiet performance problems are discussed as types of model drift; data drift can occur when the statistical distribution of an input feature changes, or concept drift occurs when there is change in the relationship between the input features and the outcome.\nWithout monitoring for degradation, this silent failure can continue undiagnosed. The vetiver framework offers functions to fluently compute, store, and plot model metrics. These functions are particularly suited to monitoring your model using multiple performance metrics over time. Effective model monitoring is not “one size fits all”, but instead depends on choosing appropriate metrics and time aggregation for a given application.",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#build-a-model",
    "href": "get-started/monitor.html#build-a-model",
    "title": "Monitor",
    "section": "Build a model",
    "text": "Build a model\n\nRPython\n\n\n\n\nShow the code from previous steps\nlibrary(pins)\nlibrary(vetiver)\nlibrary(workflows)\n\nmodel_board &lt;- board_folder(path = \"pins-r\")\nv &lt;- vetiver_pin_read(model_board, \"cars_mpg\")\n\n\n\n\n\n\nShow the code from previous steps\nfrom vetiver import VetiverModel\nfrom pins import board_folder\n\nmodel_board = board_folder(\"pins-py\", allow_pickle_read=True)\nv = VetiverModel.from_pin(model_board, \"cars_mpg\")",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#compute-metrics",
    "href": "get-started/monitor.html#compute-metrics",
    "title": "Monitor",
    "section": "Compute metrics",
    "text": "Compute metrics\nLet’s say we collect new data on fuel efficiency in cars and we want to monitor the performance of our model over time.\nWhen a model is deployed, new data comes in over time, even if time is not a feature for prediction. Even if your model does not explicitly use any dates as features, changes (or “drift”) in your machine learning system mean that your model performance can change with time.\n\n\n\n\n\n\nHow does my model use time?\n\n\n\n\nYour model sometimes uses date-time quantities as features for prediction.\nMonitoring always involves a date-time quantity, not necessarily as a feature, but as a dimension along which you are monitoring.\n\n\n\nWe can compute multiple metrics at once over a certain time aggregation.\n\nRPython\n\n\n\nlibrary(vetiver)\nlibrary(tidyverse)\ncars &lt;- read_csv(\"https://vetiver.posit.co/get-started/new-cars.csv\")\noriginal_cars &lt;- slice(cars, 1:14)\n\noriginal_metrics &lt;-\n    augment(v, new_data = original_cars) %&gt;%\n    vetiver_compute_metrics(date_obs, \"week\", mpg, .pred)\n\noriginal_metrics\n\n# A tibble: 6 × 5\n  .index        .n .metric .estimator .estimate\n  &lt;date&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2022-03-24     7 rmse    standard       4.03 \n2 2022-03-24     7 rsq     standard       0.544\n3 2022-03-24     7 mae     standard       3.05 \n4 2022-03-31     7 rmse    standard       5.69 \n5 2022-03-31     7 rsq     standard       0.651\n6 2022-03-31     7 mae     standard       3.95 \n\n\n\n\n\nimport vetiver\n\nimport pandas as pd\nfrom sklearn import metrics\nfrom datetime import timedelta\n\ncars = pd.read_csv(\"https://vetiver.posit.co/get-started/new-cars.csv\")\noriginal_cars = cars.iloc[:14, :].copy()\noriginal_cars[\"preds\"] = v.model.predict(\n    original_cars.drop(columns=[\"date_obs\", \"mpg\"])\n)\n\nmetric_set = [metrics.mean_absolute_error, metrics.mean_squared_error, metrics.r2_score]\n\ntd = timedelta(weeks=1)\n\noriginal_metrics = vetiver.compute_metrics(\n    data=original_cars,\n    date_var=\"date_obs\",\n    period=td,\n    metric_set=metric_set,\n    truth=\"mpg\",\n    estimate=\"preds\",\n)\n\noriginal_metrics\n\n       index  n               metric   estimate\n0 2022-03-24  7  mean_absolute_error   3.564286\n1 2022-03-24  7   mean_squared_error  14.804849\n2 2022-03-24  7             r2_score   0.226989\n3 2022-03-31  7  mean_absolute_error   1.732089\n4 2022-03-31  7   mean_squared_error   6.847980\n5 2022-03-31  7             r2_score   0.797150\n\n\n\n\n\nYou can specify which metrics to use for monitoring, and even provide your own custom metrics. You can choose appropriate metrics for what matters in your use case.",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#pin-metrics",
    "href": "get-started/monitor.html#pin-metrics",
    "title": "Monitor",
    "section": "Pin metrics",
    "text": "Pin metrics\nThe first time you pin monitoring metrics, you can write to a board as normal.\n\nRPython\n\n\n\nmodel_board %&gt;% pin_write(original_metrics, \"tree_metrics\")\n\n\n\n\nmodel_board.pin_write(original_metrics, \"tree_metrics\", type = \"csv\")\n\n\n\n\nHowever, when adding new metrics measurements to your pin as you continue to gather new data and monitor, you may have dates that overlap with those already in the pin, depending on your monitoring strategy. You can choose how to handle overlapping dates with the overwrite argument.\n\nRPython\n\n\n\n# dates overlap with existing metrics:\nnew_cars &lt;- slice(cars, -1:-7)\nnew_metrics &lt;-\n    augment(v, new_data = new_cars) %&gt;%\n    vetiver_compute_metrics(date_obs, \"week\", mpg, .pred)\n\nmodel_board %&gt;%\n    vetiver_pin_metrics(new_metrics, \"tree_metrics\", overwrite = TRUE)\n\n\n\n\n# dates overlap with existing metrics:\nnew_cars = cars.iloc[7:, :].copy()\nnew_cars[\"preds\"] = v.model.predict(\n    new_cars.drop(columns=[\"date_obs\", \"mpg\"])\n)\n\nnew_metrics = vetiver.compute_metrics(\n    data = new_cars, \n    date_var = \"date_obs\", \n    period = td, \n    metric_set = metric_set, \n    truth = \"mpg\", \n    estimate = \"preds\"\n)\n                    \nvetiver.pin_metrics(\n    model_board, \n    new_metrics, \n    \"tree_metrics\", \n    overwrite = True\n)",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#plot-metrics",
    "href": "get-started/monitor.html#plot-metrics",
    "title": "Monitor",
    "section": "Plot metrics",
    "text": "Plot metrics\nYou can visualize your set of computed metrics and your model’s performance1.\n\nRPython\n\n\n\nlibrary(ggplot2)\nmonitoring_metrics &lt;- model_board %&gt;% pin_read(\"tree_metrics\")\nvetiver_plot_metrics(monitoring_metrics) +\n    scale_size(range = c(2, 4))\n\n\n\n\n\n\n\n\n\n\n\nmonitoring_metrics = model_board.pin_read(\"tree_metrics\")\np = vetiver.plot_metrics(df_metrics = monitoring_metrics)\np.update_yaxes(matches=None)\np.show()\n\n\n\n\n\nIt doesn’t look like there is performance degradation in this small example. You can use these basic functions as composable building blocks for more sophisticated monitoring, including approaches such as equivocal zones or applicability domains.",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#build-a-dashboard",
    "href": "get-started/monitor.html#build-a-dashboard",
    "title": "Monitor",
    "section": "Build a dashboard",
    "text": "Build a dashboard\nThe vetiver package provides an R Markdown template for creating a monitoring dashboard. The template automates extracting some information from your metrics, and provides a way to extend the dashboard for a custom monitoring implementation.",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/monitor.html#footnotes",
    "href": "get-started/monitor.html#footnotes",
    "title": "Monitor",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKeep in mind that the R and Python models have different values for the decision tree hyperparameters.↩︎",
    "crumbs": [
      "Get Started",
      "Monitor"
    ]
  },
  {
    "objectID": "get-started/version.html#review-of-previous-steps",
    "href": "get-started/version.html#review-of-previous-steps",
    "title": "Version",
    "section": "Review of previous steps",
    "text": "Review of previous steps\n\nRPython\n\n\n\n\nShow the code from previous steps\nlibrary(tidymodels)\nlibrary(vetiver)\n\ncar_mod &lt;-\n    workflow(mpg ~ ., linear_reg()) %&gt;%\n    fit(mtcars)\nv &lt;- vetiver_model(car_mod, \"cars_mpg\")\n\n\n\n\n\n\nShow the code from previous steps\nfrom vetiver.data import mtcars\nfrom vetiver import VetiverModel\nfrom sklearn import linear_model\n\ncar_mod = linear_model.LinearRegression().fit(mtcars.drop(columns=\"mpg\"), mtcars[\"mpg\"])\n                 \nv = VetiverModel(car_mod, model_name = \"cars_mpg\", \n                 prototype_data = mtcars.drop(columns=\"mpg\"))",
    "crumbs": [
      "Get Started",
      "Version"
    ]
  },
  {
    "objectID": "get-started/version.html#store-and-version-your-model",
    "href": "get-started/version.html#store-and-version-your-model",
    "title": "Version",
    "section": "Store and version your model",
    "text": "Store and version your model\nYou can store and version your model by choosing a pins “board” for it. Your board for model pins can be set up to use a local folder, Posit Connect, Amazon S3, and more. When we write the vetiver model to our board, the binary model object is stored on our board together with necessary metadata, like the packages needed to make a prediction and the model’s input data prototype for checking new data at prediction time.\n\n\n\n\n\n\nNote\n\n\n\nWe’ll use a temporary board that will be automatically deleted for this demo, but for your real work, you will want to choose the best board for your particular infrastructure.\n\n\n\nRPython\n\n\nMost pins boards have versioning turned on by default, but we can turn it on explicitly for our temporary demo board.\n\nlibrary(pins)\nmodel_board &lt;- board_temp(versioned = TRUE)\nmodel_board %&gt;% vetiver_pin_write(v)\n\nTo read the vetiver model object from your board, use model_board %&gt;% vetiver_pin_read(\"cars_mpg\").\n\n\n\nfrom pins import board_temp\nfrom vetiver import vetiver_pin_write\nmodel_board = board_temp(versioned = True, allow_pickle_read = True)\nvetiver_pin_write(model_board, v)\n\nTo read the vetiver model object from your board, use VetiverModel.from_pin(model_board, \"cars_mpg\").\n\n\n\nLet’s train a new kind of model for mtcars, a decision tree instead of our original linear model.\n\nRPython\n\n\n\ncar_mod &lt;-\n    workflow(mpg ~ ., decision_tree(mode = \"regression\")) %&gt;%\n    fit(mtcars)\n\nv &lt;- vetiver_model(car_mod, \"cars_mpg\")\n\nmodel_board %&gt;% vetiver_pin_write(v)\n\n\n\n\nfrom sklearn import tree\ncar_mod = tree.DecisionTreeRegressor().fit(mtcars.drop(columns=\"mpg\"), mtcars[\"mpg\"])\n\nv = VetiverModel(car_mod, model_name = \"cars_mpg\", \n                 prototype_data = mtcars.drop(columns=\"mpg\"))\nvetiver_pin_write(model_board, v)\n\n\n\n\nBoth versions are stored, and we have access to both.\n\nRPython\n\n\n\nmodel_board %&gt;% pin_versions(\"cars_mpg\")\n\n# A tibble: 2 × 3\n  version                created             hash \n  &lt;chr&gt;                  &lt;dttm&gt;              &lt;chr&gt;\n1 20250528T162236Z-7f769 2025-05-28 16:22:36 7f769\n2 20250528T162238Z-322e0 2025-05-28 16:22:38 322e0\n\n\n\n\n\nmodel_board.pin_versions(\"cars_mpg\")\n\n              created   hash                 version\n0 2025-05-28 16:22:36  fa054  20250528T162236Z-fa054\n1 2025-05-28 16:22:38  4d7e7  20250528T162238Z-4d7e7\n\n\n\n\n\nThe primary purpose of pins is to make it easy to share data artifacts, so depending on the board you choose, your pinned vetiver model can be shareable with your collaborators.",
    "crumbs": [
      "Get Started",
      "Version"
    ]
  },
  {
    "objectID": "learn-more/parity-checklist.html",
    "href": "learn-more/parity-checklist.html",
    "title": "Function parity for R and Python",
    "section": "",
    "text": "This page is a translation guide for users interested in the operational parity between the R and Python versions of vetiver. Vetiver aims to provide users with a similar experience in each language. For a deeper understanding of each implementation, please refer to the R function reference or Python function reference.\n\n\n\n\n\n\n\n\nFunction\nR\nPython\n\n\n\n\nCreate a vetiver object for deployment of a trained model\nvetiver_model() new_vetiver_model()\nVetiverModel()\n\n\nRead and write a trained model to a board of models\nvetiver_pin_write() vetiver_pin_read()\nvetiver_pin_write() VetiverModel.from_pin()\n\n\nCreate an API to predict with a deployable vetiver_model() object\nvetiver_api() vetiver_pr_post() vetiver_pr_docs()\nVetiverAPI() VetiverAPI.vetiver_post()\n\n\nWrite a deployable API file for a vetiver model\nvetiver_write_plumber()\nwrite_app()\n\n\nDeploy a vetiver model API to Posit Connect\nvetiver_deploy_rsconnect()\ndeploy_rsconnect()\n\n\nCreate a model API endpoint object for prediction\nvetiver_endpoint()\nvetiver_endpoint()\n\n\nPost new data to a deployed model API endpoint and return predictions\npredict()\npredict()\n\n\nFully attach or load packages for making model predictions\nattach_pkgs() load_pkgs()\nload_pkgs()\n\n\nModel handler functions for API endpoint\nhandler_startup() handler_predict()\nVetiverHandler.handler_predict()\n\n\nIdentify data types for each column in an input data prototype\nmap_request_body()\n\n\n\nModel constructor methods\nvetiver_create_description() vetiver_prepare_model()\nVetiverHandler.describe()\n\n\nMetadata constructors for vetiver model object\nvetiver_meta() vetiver_create_meta()\nVetiverHandler.create_meta() vetiver_create_meta()\n\n\nCreate a vetiver input data prototype\nvetiver_ptype() vetiver_create_ptype()\nvetiver_create_ptype()\n\n\nConvert new data at prediction time using input data prototype\nvetiver_type_convert()\n\n\n\nCompute and aggregate model metrics over time for monitoring\nvetiver_compute_metrics()\ncompute_metrics()\n\n\nUpdate and save model metrics over time for monitoring\nvetiver_pin_metrics()\npin_metrics()\n\n\nPlot model metrics over time for monitoring\nvetiver_plot_metrics()\nplot_metrics()\n\n\nGenerate template for model monitoring dashboard\nvetiver_dashboard()\n\n\n\nGenerate template for model card\nUse \"vetiver_model_card\" template\nmodel_card()",
    "crumbs": [
      "Function parity for R and Python"
    ]
  },
  {
    "objectID": "learn-more/metrics-metadata.html",
    "href": "learn-more/metrics-metadata.html",
    "title": "Store model metrics as metadata",
    "section": "",
    "text": "The vetiver framework creates some metadata automatically for your trained model, such as the packages used to train your model and a description. You can also store any custom metadata you need for your particular MLOps use case, for example, the model metrics you observed while developing your model. When you store and version these metrics together with your model, you make them available to later analysis.",
    "crumbs": [
      "Store model metrics as metadata"
    ]
  },
  {
    "objectID": "learn-more/metrics-metadata.html#metrics-from-model-development",
    "href": "learn-more/metrics-metadata.html#metrics-from-model-development",
    "title": "Store model metrics as metadata",
    "section": "Metrics from model development",
    "text": "Metrics from model development\nFor this example, let’s work with data on hotel bookings to predict which hotel stays included children and which did not, using both feature engineering and model estimation. We put these two steps into a single function, such as a pipeline or workflow, and will deploy these pieces together.\n\nPythonR\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection, preprocessing, pipeline, compose\nfrom sklearn.ensemble import RandomForestClassifier\n\nnp.random.seed(500)\n\ndf = pd.read_csv(\"https://tidymodels.org/start/case-study/hotels.csv\").dropna()\ndf = df[['arrival_date', 'children', 'average_daily_rate', 'reserved_room_type', 'lead_time', 'country', 'adults']]\ndf[\"children\"] = df[\"children\"].map({\"none\": 0, \"children\": 1}).astype(\"int64\")\n\n# for monitoring example\nvalidation = df.sample(frac=0.10)\ndf = df.drop(index=validation.index).drop(columns=[\"arrival_date\"])\n\nX, y = df.drop(columns=\"children\"), df[\"children\"]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y, test_size=0.25\n)\n\ncategorical_features = [\"country\", \"reserved_room_type\"]\n\noe = compose.make_column_transformer(\n    (\n        preprocessing.OrdinalEncoder(\n            handle_unknown=\"use_encoded_value\", unknown_value=-1\n        ),\n        categorical_features,\n    ),\n    remainder=\"passthrough\",\n).fit(X_train)\nrf = RandomForestClassifier().fit(oe.transform(X_train), y_train)\nrf_pipe = pipeline.Pipeline([(\"oe\", oe), (\"rf\", rf)])\n\n\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nhotels &lt;- read_csv('https://tidymodels.org/start/case-study/hotels.csv') %&gt;%\n    mutate(across(where(is.character), as.factor))\n\nset.seed(123)\nhotel_split &lt;- initial_validation_split(hotels, strata = children)\nhotel_train &lt;- training(hotel_split)\nhotel_test  &lt;- testing(hotel_split)\n\n## to use for monitoring example:\nhotel_validation &lt;- validation(hotel_split)\n\nset.seed(234)\nrf_fit &lt;-\n    workflow(\n        children ~ average_daily_rate + reserved_room_type + \n            lead_time + country + adults, \n        rand_forest(mode = \"classification\", trees = 1e3)\n        ) %&gt;%\n    fit(hotel_train)\n\n\n\n\nNow that our model is trained, we can estimate the model performance we expect to see on new data using our testing data.\n\nPythonR\n\n\n\nfrom sklearn import metrics\n\nmetric_set = [metrics.accuracy_score, metrics.f1_score, metrics.log_loss]\n\nhotel_metrics = pd.DataFrame()\nfor metric in metric_set:\n    metric_name = str(metric.__name__)\n    metric_output = metric(y_test, rf_pipe.predict(X_test))\n    hotel_metrics = pd.concat(\n        (\n            hotel_metrics,\n            pd.DataFrame({\"name\": [metric_name],\n                          \"score\": [metric_output]}),\n        ),\n        axis=0,\n    )\n\nhotel_metrics.reset_index(inplace=True, drop=True)\nhotel_metrics\n\n             name     score\n0  accuracy_score  0.934645\n1        f1_score  0.499658\n2        log_loss  2.355647\n\n\n\n\n\nhotel_metric_set &lt;- metric_set(accuracy, mn_log_loss, f_meas)\nhotel_metrics &lt;-\n    augment(rf_fit, new_data = hotel_test) %&gt;%\n    hotel_metric_set(truth = children, estimate = .pred_class, .pred_children)\n\nhotel_metrics\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.940\n2 f_meas      binary         0.509\n3 mn_log_loss binary         0.192\n\n\n\n\n\nThere are differences in the metrics reported for R and Python because the models have different values for the random forest hyperparameters.\n\n\n\n\n\n\nNote\n\n\n\nMetrics to estimate model performance can be computed using different datasets. Notice that these are metrics computed when you are developing the model using the testing data; these tell you how well you expect the model to perform in the future. Another common way to compute metrics in MLOps use cases is when monitoring using new data; these tell you how well your model is performing in practice.",
    "crumbs": [
      "Store model metrics as metadata"
    ]
  },
  {
    "objectID": "learn-more/metrics-metadata.html#create-a-vetiver-model",
    "href": "learn-more/metrics-metadata.html#create-a-vetiver-model",
    "title": "Store model metrics as metadata",
    "section": "Create a vetiver model",
    "text": "Create a vetiver model\nNext, let’s create a deployable model object with vetiver, including our metrics computed during model development.\n\nPythonR\n\n\n\nimport vetiver\nv = vetiver.VetiverModel(\n    rf_pipe,\n    prototype_data=X_train,\n    model_name=\"hotel-rf\",\n    metadata=hotel_metrics.to_dict(),\n)\nv.description\n\n'A scikit-learn Pipeline model'\n\n\n\n\n\nlibrary(vetiver)\nv &lt;- vetiver_model(\n    rf_fit, \n    \"hotel-rf\", \n    metadata = list(metrics = hotel_metrics)\n)\nv\n\n\n── hotel-rf ─ &lt;bundled_workflow&gt; model for deployment \nA ranger classification modeling workflow using 5 features",
    "crumbs": [
      "Store model metrics as metadata"
    ]
  },
  {
    "objectID": "learn-more/metrics-metadata.html#version-your-model",
    "href": "learn-more/metrics-metadata.html#version-your-model",
    "title": "Store model metrics as metadata",
    "section": "Version your model",
    "text": "Version your model\nWe pin our vetiver model to a board to version it. The metadata, including our metrics, are versioned along with the model.\n\nPythonR\n\n\n\nfrom pins import board_temp\nfrom vetiver import vetiver_pin_write\n\nmodel_board = board_temp(versioned=True, allow_pickle_read=True)\nvetiver_pin_write(model_board, v)\n\n\n\n\nlibrary(pins)\nmodel_board &lt;- board_temp(versioned = TRUE)\nmodel_board %&gt;% vetiver_pin_write(v)\n\n\n\n\nIf we trained this model again with different data and new values for these metrics, we could store it again as a new version and have access to both sets of metrics.\n\n\n\n\n\n\nTip\n\n\n\nLike in our article on versioning, we are using a temporary board that will be automatically deleted for this demo. For your real work, you will want to choose the best board for your particular infrastructure.",
    "crumbs": [
      "Store model metrics as metadata"
    ]
  },
  {
    "objectID": "learn-more/metrics-metadata.html#extract-your-metrics-metadata",
    "href": "learn-more/metrics-metadata.html#extract-your-metrics-metadata",
    "title": "Store model metrics as metadata",
    "section": "Extract your metrics metadata",
    "text": "Extract your metrics metadata\nSo far, we have walked through how to store metadata, but how do we extract our metrics out to use them?\nYou can use pin_meta() to retrieve metadata from your board. All custom metadata is stored in a \"user\" slot; remember that other metadata is also automatically stored for you as well.\n\nPythonR\n\n\n\nmetadata = model_board.pin_meta(\"hotel-rf\")\nextracted_metrics = pd.DataFrame(metadata.user.get(\"user\"))\nextracted_metrics\n\n             name     score\n0  accuracy_score  0.934645\n1        f1_score  0.499658\n2        log_loss  2.355647\n\n\n\n\n\nextracted_metrics &lt;- \n    model_board %&gt;% \n    pin_meta(\"hotel-rf\") %&gt;% \n    pluck(\"user\", \"metrics\") %&gt;% \n    as_tibble()\n\nextracted_metrics\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.940\n2 f_meas      binary         0.509\n3 mn_log_loss binary         0.192\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse the version argument to pin_meta() to get the metadata for a specific model version.\n\n\nIf you already have your vetiver model available, you can alternatively retrieve the metadata directly:\n\nPythonR\n\n\n\npd.DataFrame(v.metadata.user)\n\n             name     score\n0  accuracy_score  0.934645\n1        f1_score  0.499658\n2        log_loss  2.355647\n\n\n\n\n\nv %&gt;% pluck(\"metadata\", \"user\", \"metrics\")\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.940\n2 f_meas      binary         0.509\n3 mn_log_loss binary         0.192\n\n\n\n\n\nNow that we have extracted_metrics, we can use them, for example, when plotting model monitoring metrics.\n\nPythonR\n\n\n\n## Use model monitoring functions together with validation data to show an example plot\nfrom datetime import timedelta\ntd = timedelta(weeks= 4)\n\nvalidation[\"preds\"] = v.model.predict(validation.drop(columns=[\"children\"]))\n\ntd_metrics = vetiver.compute_metrics(\n    data = validation, \n    date_var = \"arrival_date\", \n    period = td, \n    metric_set = metric_set, \n    truth = \"children\", \n    estimate = \"preds\"\n)\n\nfig = vetiver.plot_metrics(td_metrics)\n\n# plotly rows are counted from 3 to 1\nfor i, j in zip(range(3), range(3, 0, -1)):\n    metric = extracted_metrics.iloc[i,1]\n    annotation = extracted_metrics.iloc[i,0]\n    fig = fig.add_hline(y=metric, line_dash=\"dot\", row=j)\n\nfig = fig.update_layout(xaxis_title=None)\nfig = fig.update_yaxes(matches=None, title = None)\nfig.show()\n\n\n\nWarning: `includeHTML()` was provided a `path` that appears to be a complete HTML document.\n✖ Path: metrics-metadata-python.html\nℹ Use `tags$iframe()` to include an HTML document. You can either ensure `path` is accessible in your app or document (see e.g. `shiny::addResourcePath()`) and pass the relative path to the `src` argument. Or you can read the contents of `path` and pass the contents to `srcdoc`.\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n## use validation data for monitoring example:\naugment(rf_fit, new_data = hotel_validation) %&gt;%\n    arrange(arrival_date) %&gt;%\n    vetiver_compute_metrics(arrival_date, \"month\", \n                            children, .pred_class, .pred_children,\n                            metric_set = hotel_metric_set) %&gt;%\n    vetiver_plot_metrics() +\n    geom_hline(aes(yintercept = .estimate, color = .metric), \n               data = extracted_metrics,\n               linewidth = 1.5, alpha = 0.7, lty = 2)",
    "crumbs": [
      "Store model metrics as metadata"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MLOps with vetiver",
    "section": "",
    "text": "Machine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently. The vetiver framework is for MLOps tasks in Python and R.\n\nVetiver, the oil of tranquility, is used as a stabilizing ingredient in perfumery to preserve more volatile fragrances.\n\nThe goal of vetiver is to provide fluent tooling to version, deploy, and monitor a trained model. Functions handle both recording and checking the model’s input data prototype, and predicting from a remote API endpoint.\n\n\n\n\n\n\n\n\n\n\n\nData scientists have effective tools that they ❤️ to:\n\n\n\n\ncollect data\nprepare, manipulate, refine data\ntrain models\n\n\n\n\n\n\n\n\n\nThere is a lack 😩 of effective tools to:\n\n\n\n\nversion and publish models\nput models into production\nmonitor model performance\n\n\n\nUse vetiver to version and deploy your trained models.\n\nRPython\n\n\n\nlibrary(vetiver)\ncars_lm &lt;- lm(mpg ~ ., data = mtcars)\nvetiver_model(cars_lm, \"cars_linear\")\n\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n\n\n\n── cars_linear ─ &lt;butchered_lm&gt; model for deployment \nAn OLS linear regression model using 10 features\n\n\n\n\n\nfrom vetiver import VetiverModel\nfrom vetiver.data import mtcars\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression().fit(mtcars.drop(columns=\"mpg\"), mtcars[\"mpg\"])\nv = VetiverModel(model, model_name = \"cars_linear\", \n                 prototype_data = mtcars.drop(columns=\"mpg\"))\nv.description\n\n'A scikit-learn LinearRegression model'"
  },
  {
    "objectID": "about.html#what-is-mlops",
    "href": "about.html#what-is-mlops",
    "title": "About vetiver",
    "section": "What is MLOps?",
    "text": "What is MLOps?\nMachine learning operations, or MLOps, is a set of practices to deploy and maintain machine learning models in production reliably and efficiently."
  },
  {
    "objectID": "about.html#what-tasks-are-involved-in-mlops-practices",
    "href": "about.html#what-tasks-are-involved-in-mlops-practices",
    "title": "About vetiver",
    "section": "What tasks are involved in MLOps practices?",
    "text": "What tasks are involved in MLOps practices?\nSome MLOps tasks are data versioning, tuning/training models, experiment tracking, model versioning, model deployment, model monitoring, and workflow orchestration. The vetiver framework focuses on the tasks of versioning, deploying, and monitoring an ML model."
  },
  {
    "objectID": "about.html#how-can-i-tell-if-i-am-on-the-right-track-while-building-an-mlops-system",
    "href": "about.html#how-can-i-tell-if-i-am-on-the-right-track-while-building-an-mlops-system",
    "title": "About vetiver",
    "section": "How can I tell if I am on the right track while building an MLOps system?",
    "text": "How can I tell if I am on the right track while building an MLOps system?\nYou can use the rubric in “The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction” by Breck et al to understand how ready a machine learning system is for production. This rubric outlines 28 specific tests plus a grading scale to score how robust a deployment strategy is."
  },
  {
    "objectID": "about.html#how-does-vetiver-compare-to-other-mlops-tools",
    "href": "about.html#how-does-vetiver-compare-to-other-mlops-tools",
    "title": "About vetiver",
    "section": "How does vetiver compare to other MLOps tools?",
    "text": "How does vetiver compare to other MLOps tools?\n\n😌 vetiver is a focused framework for data practitioners\nThe vetiver framework does not aim to be an all-in-one solution for all parts of the MLOps life cycle. Instead, vetiver provides tooling to version, deploy, and monitor ML models. This focus allows vetiver to provide a better user experience and composability as organizations mature in their MLOps needs. Tools like MLFlow and managed cloud offerings like SageMaker and AzureML are designed to be comprehensive; this can be the right choice in some situations but comes with usability challenges and a steeper learning curve.\nThe vetiver framework is built to be used by practitioners like data scientists and data analysts, the people who develop models. We believe that the person who develops a model is the right person to operationalize that model. This is in contrast to MLFlow, which is most comfortable for a software engineering persona (rather than data analysis persona).\n\n\n🐍 vetiver supports both Python and R\nExisting tools for MLOps tasks overwhelmingly support Python only, or provide poor or incomplete support other languages like R. Even cloud offerings like SageMaker and AzureML have very few meaningful native options for practitioners that want flexibility in their ML choices, with the ability to use their model implementation of choice for a specific problem. Data science teams that use both R and Python are an important focus for Posit, and we aim to provide the best possible tools for these bilingual teams.\nThe design of vetiver can be extended in the future to other languages. The underlying technologies (REST APIs, binary storage, dashboards) are language agnostic and could be extended to, for example, Julia.\n\n\n🚀 vetiver is for getting started with MLOps\nIn terms of MLOps maturity, it is still early days for many organizations and practitioners. These folks need a tool that provides a fluent experience as they get started with their MLOps tasks and grow their institutional knowledge about model deployment and management. The vetiver framework is uniquely positioned for teams getting started with MLOps, in terms of our attention to user experience and focus on documentation.\nOther existing tools like BentoML are similar to vetiver in that they are focused solutions built for practitioners, but BentoML provides advanced functionality and is less appropriate for a team just getting started."
  },
  {
    "objectID": "about.html#who-is-developing-vetiver",
    "href": "about.html#who-is-developing-vetiver",
    "title": "About vetiver",
    "section": "Who is developing vetiver?",
    "text": "Who is developing vetiver?\nDevelopment of vetiver is sponsored by Posit PBC."
  },
  {
    "objectID": "about.html#is-vetiver-open-source",
    "href": "about.html#is-vetiver-open-source",
    "title": "About vetiver",
    "section": "Is vetiver open source?",
    "text": "Is vetiver open source?\nThe vetiver Python and R packages are released under the MIT license."
  },
  {
    "objectID": "about.html#what-are-different-ways-you-can-contribute",
    "href": "about.html#what-are-different-ways-you-can-contribute",
    "title": "About vetiver",
    "section": "What are different ways you can contribute?",
    "text": "What are different ways you can contribute?\n\nAnswer questions\nYou can help others use and learn vetiver by answering questions on the Posit community site, Stack Overflow, and Twitter. Many people asking for help with vetiver don’t know what a reproducible example or “reprex” is, or how to craft one. Acknowledging an individual’s problem, showing them how to build a reprex, and pointing them to helpful resources are all enormously beneficial, even if you don’t immediately solve their problem.\nRemember that while you might have seen a problem a hundred times before, it’s new to the person asking it. Be patient, polite, and empathetic.\n\n\nFile issues\nIf you’ve found a bug, first create a minimal reproducible example. Spend some time working to make it as minimal as possible; the more time you spend doing this, the easier it is to fix the bug. When your reprex is ready, file it on the GitHub repo of the appropriate package, either Python or R.\nThe vetiver team often focuses on one package at a time to reduce context switching and be more efficient. We may not address each issue right away, but we will use the reproducible example you create to understand your problem when it is time to focus on that package.\n\n\nContribute documentation\nDocumentation is a high priority for vetiver, and pull requests to correct or improve documentation are welcome.\n\n\nContribute code\nIf you are a more experienced R or Python programmer, you may have the inclination, interest, and ability to contribute directly to package development. Before you submit a pull request to vetiver, always file an issue and confirm the vetiver team agrees with your idea and is happy with your basic proposal.\nWe use the tidyverse style guide for R and the PEP 8 style guide for Python. Using a style guide keeps your new code and documentation matching the existing style, and makes the review process much smoother."
  },
  {
    "objectID": "learn-more/deploy-with-docker.html",
    "href": "learn-more/deploy-with-docker.html",
    "title": "Deploy with Docker",
    "section": "",
    "text": "If you plan to bring vetiver to a public or private cloud rather than Posit Connect, Docker containers are a highly portable solution. Using vetiver makes Dockerfile creation easy by generating the files you need from your trained models.",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#import-data",
    "href": "learn-more/deploy-with-docker.html#import-data",
    "title": "Deploy with Docker",
    "section": "Import data",
    "text": "Import data\nFor this demo, we will use data from Tidy Tuesday to predict the number of YouTube likes a television commercial played during the Super Bowl will get, based on qualities such as if the ad included any animals, if the ad was funny, if the ad had any elements of danger, etc.\n\nPythonR\n\n\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(500)\n\nraw = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')\ndf = pd.DataFrame(raw)\n\ndf = df[[\"like_count\", \"funny\", \"show_product_quickly\", \"patriotic\", \\\n    \"celebrity\", \"danger\", \"animals\"]].dropna()\n\ndf.head(3)\n\n   like_count  funny  show_product_quickly  ...  celebrity  danger  animals\n0      1233.0  False                 False  ...      False   False    False\n1       485.0   True                  True  ...       True    True    False\n2       129.0   True                 False  ...      False    True     True\n\n[3 rows x 7 columns]\n\n\n\n\n\nlibrary(tidyverse)\nsuperbowl_ads_raw &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')\n\nsuperbowl_ads &lt;-\n    superbowl_ads_raw %&gt;%\n    select(funny:animals, like_count) %&gt;%\n    na.omit()\n\nsuperbowl_ads\n\n# A tibble: 225 × 7\n   funny show_product_quickly patriotic celebrity danger animals like_count\n   &lt;lgl&gt; &lt;lgl&gt;                &lt;lgl&gt;     &lt;lgl&gt;     &lt;lgl&gt;  &lt;lgl&gt;        &lt;dbl&gt;\n 1 FALSE FALSE                FALSE     FALSE     FALSE  FALSE         1233\n 2 TRUE  TRUE                 FALSE     TRUE      TRUE   FALSE          485\n 3 TRUE  FALSE                FALSE     FALSE     TRUE   TRUE           129\n 4 FALSE TRUE                 FALSE     FALSE     FALSE  FALSE            2\n 5 TRUE  TRUE                 FALSE     FALSE     TRUE   TRUE            20\n 6 TRUE  TRUE                 FALSE     TRUE      TRUE   TRUE           115\n 7 TRUE  FALSE                FALSE     TRUE      FALSE  TRUE          1470\n 8 FALSE FALSE                FALSE     TRUE      FALSE  FALSE           78\n 9 TRUE  TRUE                 FALSE     TRUE      FALSE  TRUE           342\n10 FALSE TRUE                 TRUE      TRUE      TRUE   FALSE            7\n# ℹ 215 more rows",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#build-a-model",
    "href": "learn-more/deploy-with-docker.html#build-a-model",
    "title": "Deploy with Docker",
    "section": "Build a model",
    "text": "Build a model\nWith data in hand, the next step is feature engineering and model estimation. We put these two steps into a single function, such as a pipeline or workflow, and will deploy these pieces together. (Why are we doing this?)\n\nPythonR\n\n\n\nfrom sklearn import model_selection, preprocessing, pipeline\nfrom sklearn.ensemble import RandomForestRegressor\n\nX, y = df.iloc[:,1:],df['like_count']\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X, y,\n    test_size=0.2\n)\n\nle = preprocessing.OrdinalEncoder().fit(X)\nrf = RandomForestRegressor().fit(le.transform(X_train), y_train)\nrf_pipe = pipeline.Pipeline([('label_encoder',le), ('random_forest', rf)])\n\n\n\n\nlibrary(tidymodels)\n\nrf_spec &lt;- rand_forest(mode = \"regression\")\nrf_form &lt;- like_count ~ .\n\nrf_fit &lt;-\n    workflow(rf_form, rf_spec) %&gt;%\n    fit(superbowl_ads)",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#create-a-vetiver-model",
    "href": "learn-more/deploy-with-docker.html#create-a-vetiver-model",
    "title": "Deploy with Docker",
    "section": "Create a vetiver model",
    "text": "Create a vetiver model\nNext, let’s create a deployable model object with vetiver.\n\nPythonR\n\n\n\nimport vetiver\n\nv = vetiver.VetiverModel(\n    rf_pipe, \n    prototype_data=X_train, \n    model_name = \"superbowl_rf\"\n)\nv.description\n\n'A scikit-learn Pipeline model'\n\n\n\n\n\nlibrary(vetiver)\n\nv &lt;- vetiver_model(rf_fit, \"superbowl_rf\")\nv\n\n\n── superbowl_rf ─ &lt;bundled_workflow&gt; model for deployment \nA ranger regression modeling workflow using 6 features",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#version-your-model",
    "href": "learn-more/deploy-with-docker.html#version-your-model",
    "title": "Deploy with Docker",
    "section": "Version your model",
    "text": "Version your model\nWe pin our vetiver model to a board to version it. We will also use this board later to create artifacts for our Dockerfile.\n\nPythonR\n\n\n\nimport pins\n\nboard = pins.board_rsconnect(\n    server_url=server_url, # load from an .env file\n    api_key=api_key, # load from an .env file \n    allow_pickle_read=True\n)\n\nvetiver.vetiver_pin_write(board, v)\n\n\n\n\nlibrary(pins)\nboard &lt;- board_connect() # authenticates via environment variables\nvetiver_pin_write(board, v)\n\n\n\n\nHere we are using board_connect(), but you can use other boards such as board_s3(). Read more about how to store and version your vetiver model.\n\n\n\n\n\n\nUsing local boards with Docker\n\n\n\n\n\nLocal boards such as board_folder() will not be immediately available to Docker images created by vetiver. We don’t recommend that you store your model inside your container, but (if appropriate to your use case) it is possible to edit the generated Dockerfile and COPY the folder and model into the container. Alternatively, you can mount the folder as a VOLUME.\nLearn more about why we recommend storing your versioned model binaries outside Docker containers in this talk.",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#create-docker-artifacts",
    "href": "learn-more/deploy-with-docker.html#create-docker-artifacts",
    "title": "Deploy with Docker",
    "section": "Create Docker artifacts",
    "text": "Create Docker artifacts\nTo build a Docker image that can serve your model, you need three artifacts:\n\nthe Dockerfile itself,\na requirements.txt or renv.lock to capture your model dependencies, and\nan app.py or plumber.R file containing the information to serve a vetiver REST API.\n\nYou can create all the needed files with one function.\n\nPythonR\n\n\n\nvetiver.prepare_docker(\n    board, \n    \"isabel.zimmerman/superbowl_rf\",\n    version = \"20220901T144702Z-fd402\",\n    port = 8080\n)\n\n\n\n\nvetiver_prepare_docker(\n    board, \n    \"julia.silge/superbowl_rf\", \n    docker_args = list(port = 8080)\n)\n\n\n\n\nYou have now created all the files needed to build your Docker image!",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#build-and-run-your-dockerfile",
    "href": "learn-more/deploy-with-docker.html#build-and-run-your-dockerfile",
    "title": "Deploy with Docker",
    "section": "Build and run your Dockerfile",
    "text": "Build and run your Dockerfile\nIt is time to build and run your container. Building the Docker container can potentially take a while, because it installs all the packages needed to make a prediction with this model. Use the command line (not R or Python) to build your Docker container:\n\ndocker build -t superbowlads .\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are on an ARM architecture locally and deploying an R model, use --platform linux/amd64 for RSPM’s fast installation of R package binaries.\n\n\nNow run! To authenticate to your board (to get the pinned vetiver model from, for example, Posit Connect), pass in a file supplying environment variables.\n\ndocker run --env-file .env -p 8080:8080 superbowlads\n\n\n\n\n\n\n\nTip\n\n\n\nR users likely will store their environment variables in a file called .Renviron instead of .env.\n\n\nThe Docker container is now running locally! You can interact with it, such as by using a browser to visit http://0.0.0.0:8080/__docs__/",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#make-predictions-from-docker-container",
    "href": "learn-more/deploy-with-docker.html#make-predictions-from-docker-container",
    "title": "Deploy with Docker",
    "section": "Make predictions from Docker container",
    "text": "Make predictions from Docker container\nRunning a Docker container locally is a great way to test that you can make predictions from your endpoint as expected, using R or Python.\n\nPythonR\n\n\n\nendpoint = vetiver.vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\nvetiver.predict(endpoint=endpoint, data=X_test)\n\n\n\n\nnew_ads &lt;- superbowl_ads %&gt;% \n    select(-like_count)\n\nendpoint &lt;- vetiver_endpoint(\"http://0.0.0.0:8080/predict\")\n\npredict(endpoint, new_ads)\n\n\n\n\nWhen you’re done, stop all Docker containers from the command line with:\n\ndocker stop $(docker ps -a -q)",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/deploy-with-docker.html#what-if-i-dont-know-how-to-use-docker",
    "href": "learn-more/deploy-with-docker.html#what-if-i-dont-know-how-to-use-docker",
    "title": "Deploy with Docker",
    "section": "What if I don’t know how to use Docker?",
    "text": "What if I don’t know how to use Docker?\nDocker is a great tool for data scientists, so learning the basics is a good idea. These resources can help you get started:\n\nEnough Docker to be Dangerous\nDocker for the UseR\nDocker tutorial for reproducible research\nPython Docker\nTen simple rules for writing Dockerfiles for reproducible data science\nDocker and Python: making them play nicely and securely for Data Science and ML\nDocker info from Posit Solutions Engineering",
    "crumbs": [
      "Deploy with Docker"
    ]
  },
  {
    "objectID": "learn-more/model-card.html#what-is-a-model-card",
    "href": "learn-more/model-card.html#what-is-a-model-card",
    "title": "Model cards for transparent, responsible reporting",
    "section": "What is a “Model Card”?",
    "text": "What is a “Model Card”?\nGood documentation helps us make sense of software, know when and how to use it, and understand its purpose. The same can be true of documentation or reporting for a deployed model, but it can be hard to know where to start. The paper “Model Cards for Model Reporting” (Mitchell et al. 2019) provides a suggested framework for organizing and presenting the essential facts about a deployed machine learning model. The vetiver package provides an R Markdown template for creating a “Model Card” for a published vetiver model. The template automates extracting some information from the model object, and provides structure for the model developer where automation is not possible.\nModel developers see a nudge to create a model card when they publish a model; we recommend that you create a model card when you deploy a model for the first time and refresh that model card as needed when new versions are deployed.\n\nRPython\n\n\n\nlibrary(vetiver)\nlibrary(pins)\nmodel_board &lt;- board_temp()\n\ncars_lm &lt;- lm(mpg ~ ., data = mtcars)\nv &lt;- vetiver_model(cars_lm, \"cars_linear\")\n\nRegistered S3 method overwritten by 'butcher':\n  method                 from    \n  as.character.dev_topic generics\n\nvetiver_pin_write(model_board, v)\n\nCreating new version '20250528T162221Z-fbaec'\n\n\nWriting to pin 'cars_linear'\n\nCreate a Model Card for your published model\n• Model Cards provide a framework for transparent, responsible reporting\n• Use the vetiver `.Rmd` template as a place to start\n\n\n(Learn more about silencing messages like this if desired.)\n\n\n\nimport vetiver\nimport pins\nfrom vetiver.data import mtcars\nfrom sklearn.linear_model import LinearRegression\n\nmodel_board = pins.board_temp(allow_pickle_read=True)\n\nmodel = LinearRegression().fit(mtcars.drop(columns=\"mpg\"), mtcars[\"mpg\"])\nv = vetiver.VetiverModel(model, model_name = \"cars_linear\", \n                         prototype_data = mtcars.drop(columns=\"mpg\"))\n\nvetiver.vetiver_pin_write(model_board, v)\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\nWriting pin:\nName: 'cars_linear'\nVersion: 20250528T162223Z-fa054\n\n\nTo silence this message,\nfrom vetiver.utils import modelcard_options\n\nmodelcard_options.quiet = True",
    "crumbs": [
      "Model cards for transparent, responsible reporting"
    ]
  },
  {
    "objectID": "learn-more/model-card.html#accessing-the-template",
    "href": "learn-more/model-card.html#accessing-the-template",
    "title": "Model cards for transparent, responsible reporting",
    "section": "Accessing the template",
    "text": "Accessing the template\n\nRPython\n\n\nTo use the vetiver model card template from RStudio, access through File -&gt; New File -&gt; R Markdown. This will open the dialog box where you can select from one of the available templates:\n\n\n\n\n\nIf you are not using RStudio, you’ll also need to install Pandoc. Then, use the rmarkdown::draft() function to create the model card:\nrmarkdown::draft(\n    \"my_model_card.Rmd\", \n    template = \"vetiver_model_card\", \n    package = \"vetiver\"\n)\n\n\nTo use a model card template with Python, you will need to have Quarto installed. Then, use the vetiver.model_card() function to generate the template.\nvetiver.model_card(path = \".\")",
    "crumbs": [
      "Model cards for transparent, responsible reporting"
    ]
  },
  {
    "objectID": "learn-more/model-card.html#model-card-outline",
    "href": "learn-more/model-card.html#model-card-outline",
    "title": "Model cards for transparent, responsible reporting",
    "section": "Model card outline",
    "text": "Model card outline\nThere are several sections in the model card framework used here.\n\nModel details: Some details about your model can be determined from the model object itself, but some (like who developed the model and license or citation information) need to be provided by you.\nIntended use: Outline the intended use and users of the model, and perhaps also what types of use would be out of scope.\nImportant aspects/factors: What are the demographic, environmental, technical, or other aspects that are relevant to the context of the model?\nPerformance metrics: Communicate which metrics are being used to evaluate the model, and why these are a good fit for the model’s context and domain.\nTraining data & evaluation data: Some specific dataset was used to train the model, so be sure to share basic details about its characteristics. (Some information about the training data can be extracted from the model object itself.) Some (probably different) specific dataset is used to evaluate the model in the context of the model card, so also explain what the evaluation data is like.\nQuantitative analyses: Provide the results of evaluating the model using your chosen metrics and the evaluation data. Be sure to present both overall results (for the dataset as a whole) and disaggregated results, especially with any aspects (demographic, environmental, or other) in mind that have been identified as important for this model. You can use both tables and visualization to present these quantitative analyses.\nEthical considerations: Share ethical considerations and any possible solutions considered. Some specific aspects to note are any sensitive data used, impact on human life, possible risks and harms, and important use cases.\nCaveats & recommendations: As the model developer, you likely have the most domain knowledge of what the model can and cannot do. This section is a good place to share any additional thoughts, perhaps including how your own identity may or may not come into play in the model’s context.",
    "crumbs": [
      "Model cards for transparent, responsible reporting"
    ]
  },
  {
    "objectID": "learn-more/model-card.html#cant-i-just-delete-the-section-on-ethical-considerations",
    "href": "learn-more/model-card.html#cant-i-just-delete-the-section-on-ethical-considerations",
    "title": "Model cards for transparent, responsible reporting",
    "section": "Can’t I just delete the section on ethical considerations?",
    "text": "Can’t I just delete the section on ethical considerations?\nIt’s possible that a given machine learning model may not have obvious caveats, ethical challenges, or demographic aspects, or that they are largely unknown. However, we strongly advise that instead of deleting any such section because you have incomplete or imprecise information, you note your own process and considerations. Also, consider the possibility of gathering feedback from those impacted by the machine learning system, especially those with marginalized identities.\nThe process of documenting the extent and limits of a machine learning system is part of transparent, responsible reporting. A model card framework such as this is a helpful tool and some parts of a model card can be automated, but ultimately the extent of its value depends on you. From Mitchell et al. (2019):\n\nTherefore the usefulness and accuracy of a model card relies on the integrity of the creator(s) of the card itself.",
    "crumbs": [
      "Model cards for transparent, responsible reporting"
    ]
  },
  {
    "objectID": "get-started/index.html",
    "href": "get-started/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "The vetiver framework for MLOps tasks is built for data science teams using R and/or Python, with a native, fluent experience for both. It is built to be extensible, with methods that can support many kinds of models.",
    "crumbs": [
      "Get Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#installation",
    "href": "get-started/index.html#installation",
    "title": "Getting Started",
    "section": "Installation",
    "text": "Installation\n\nRPython\n\n\nYou can use vetiver with:\n\na tidymodels workflow (including stacks)\ncaret\nmlr3\nXGBoost\nranger\nlm() and glm()\nGAMS fit with mgcv\n\nYou can install the released version of vetiver from CRAN:\n\ninstall.packages(\"vetiver\")\n\nAnd the development version from GitHub with:\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"tidymodels/vetiver-r\")\n\n\n\nYou can use vetiver with:\n\nscikit-learn\nPyTorch\nXGBoost\nstatsmodels\n\nYou can install the released version of vetiver from PyPI:\n\npython -m pip install vetiver\n\nAnd the development version from GitHub with:\n\npython -m pip install git+https://github.com/rstudio/vetiver-python",
    "crumbs": [
      "Get Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#train-a-model",
    "href": "get-started/index.html#train-a-model",
    "title": "Getting Started",
    "section": "Train a model",
    "text": "Train a model\nFor this example, let’s work with data on fuel efficiency for cars to predict miles per gallon.\n\nRPython\n\n\nLet’s consider one kind of model supported by vetiver, a tidymodels workflow that encompasses both feature engineering and model estimation.\n\nlibrary(tidymodels)\n\ncar_mod &lt;-\n    workflow(mpg ~ ., linear_reg()) %&gt;%\n    fit(mtcars)\n\n\n\nLet’s consider one kind of model supported by vetiver, a scikit-learn linear model.\n\nfrom vetiver.data import mtcars\nfrom sklearn.linear_model import LinearRegression\n\ncar_mod = LinearRegression().fit(mtcars.drop(columns=\"mpg\"), mtcars[\"mpg\"])\n\n\n\n\nThis car_mod object is a fitted model, with model parameters estimated using mtcars.",
    "crumbs": [
      "Get Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/index.html#create-a-vetiver-model",
    "href": "get-started/index.html#create-a-vetiver-model",
    "title": "Getting Started",
    "section": "Create a vetiver model",
    "text": "Create a vetiver model\nWe can create a vetiver_model() in R or VetiverModel() in Python from the trained model; a vetiver model object collects the information needed to store, version, and deploy a trained model.\n\nRPython\n\n\n\nlibrary(vetiver)\nv &lt;- vetiver_model(car_mod, \"cars_mpg\")\nv\n\n\n── cars_mpg ─ &lt;bundled_workflow&gt; model for deployment \nA lm regression modeling workflow using 10 features\n\n\n\n\n\nfrom vetiver import VetiverModel\nv = VetiverModel(car_mod, model_name = \"cars_mpg\", \n                 prototype_data = mtcars.drop(columns=\"mpg\"))\nv.description\n\n'A scikit-learn LinearRegression model'\n\n\n\n\n\nThink of this vetiver model as a deployable model object.",
    "crumbs": [
      "Get Started",
      "Getting Started"
    ]
  },
  {
    "objectID": "get-started/deploy.html#review-of-previous-steps",
    "href": "get-started/deploy.html#review-of-previous-steps",
    "title": "Deploy",
    "section": "Review of previous steps",
    "text": "Review of previous steps\n\nRPython\n\n\n\n\nShow the code from previous steps\nlibrary(tidymodels)\nlibrary(vetiver)\nlibrary(pins)\n\ncar_mod &lt;-\n    workflow(mpg ~ ., decision_tree(mode = \"regression\")) %&gt;%\n    fit(mtcars)\nv &lt;- vetiver_model(car_mod, \"cars_mpg\")\nmodel_board &lt;- board_folder(\"pins-r\", versioned = TRUE)\nmodel_board %&gt;% vetiver_pin_write(v)\n\n\n\n\n\n\nShow the code from previous steps\nfrom vetiver.data import mtcars\nfrom vetiver import VetiverModel, vetiver_pin_write\nfrom sklearn import tree\nfrom pins import board_folder\n\ncar_mod = tree.DecisionTreeRegressor().fit(mtcars.drop(columns=\"mpg\"), mtcars[\"mpg\"])\n\nv = VetiverModel(car_mod, model_name = \"cars_mpg\", \n                 prototype_data = mtcars.drop(columns=\"mpg\"))\n\nmodel_board = board_folder(\"pins-py\", allow_pickle_read=True)\nvetiver_pin_write(model_board, v)",
    "crumbs": [
      "Get Started",
      "Deploy"
    ]
  },
  {
    "objectID": "get-started/deploy.html#create-a-rest-api-for-deployment",
    "href": "get-started/deploy.html#create-a-rest-api-for-deployment",
    "title": "Deploy",
    "section": "Create a REST API for deployment",
    "text": "Create a REST API for deployment\nYou can deploy your model by creating a special Plumber router in R or a FastAPI router in Python, and adding a POST endpoint for making predictions.\n\nRPython\n\n\n\nlibrary(plumber)\npr() %&gt;%\n  vetiver_api(v)\n\n# Plumber router with 4 endpoints, 4 filters, and 1 sub-router.\n# Use `pr_run()` on this object to start the API.\n├──[queryString]\n├──[body]\n├──[cookieParser]\n├──[sharedSecret]\n├──/logo\n│  │ # Plumber static router serving from directory: /home/runner/work/_temp/Library/vetiver\n├──/metadata (GET)\n├──/ping (GET)\n├──/predict (POST)\n└──/prototype (GET)\n\n\nTo start a server using this object, pipe (%&gt;%) to pr_run(port = 8080) or your port of choice.\n\n\n\nfrom vetiver import VetiverAPI\napp = VetiverAPI(v, check_prototype=True)\n\nTo start a server using this object, use app.run(port = 8080) or your port of choice.\n\n\n\nYou can interact with your vetiver API via automatically generated, detailed visual documentation.\n\n\n\n\n\n\n\n\n\n\n\n\nFastAPI and Plumber APIs such as these can be hosted in a variety of ways. Let’s walk through two options: deploying to Posit Connect or with Docker.",
    "crumbs": [
      "Get Started",
      "Deploy"
    ]
  },
  {
    "objectID": "get-started/deploy.html#deploy-to-connect",
    "href": "get-started/deploy.html#deploy-to-connect",
    "title": "Deploy",
    "section": "Deploy to Connect",
    "text": "Deploy to Connect\nFor Posit Connect, you can deploy your versioned model with a single function.\n\nRPython\n\n\n\n# authenticates via environment variables:\nvetiver_deploy_rsconnect(model_board, \"user.name/cars_mpg\")\n\n\n\n\nfrom rsconnect.api import RSConnectServer\n\nconnect_server = RSConnectServer(\n    url=server_url, # load from an .env file\n    api_key=api_key # load from an .env file \n)\n\nvetiver.deploy_rsconnect(\n    connect_server = connect_server,\n    board = model_board,\n    pin_name = \"user.name/cars_mpg\",\n)\n\n\n\n\nIn this case, you probably want model_board to be a Connect pins board (board_connect()). For more on deploying to Connect, see the Connect documentation for using vetiver.",
    "crumbs": [
      "Get Started",
      "Deploy"
    ]
  },
  {
    "objectID": "get-started/deploy.html#prepare-a-dockerfile",
    "href": "get-started/deploy.html#prepare-a-dockerfile",
    "title": "Deploy",
    "section": "Prepare a Dockerfile",
    "text": "Prepare a Dockerfile\nFor deploying a vetiver API to infrastructure other than Posit Connect, such as Google Cloud Run, AWS, or Azure, you likely will want to build a Docker container.\n\n\n\n\n\n\nNote\n\n\n\nYou can use any pins board with Docker, like board_folder() or board_connect(), as long as your Docker container can authenticate to your pins board.\n\n\n\nRPython\n\n\n\nvetiver_prepare_docker(model_board, \"cars_mpg\")\n\n\n\nThe following required packages are not installed:\n- cpp11     [required by clock, lobstr, readr, and 4 others]\n- progress  [required by vroom]\nConsider reinstalling these packages before snapshotting the lockfile.\n\n\n# Generated by the vetiver package; edit with care\n\nFROM rocker/r-ver:4.5.0\nENV RENV_CONFIG_REPOS_OVERRIDE https://packagemanager.rstudio.com/cran/latest\n\nRUN apt-get update -qq && apt-get install -y --no-install-recommends \\\n  libcurl4-openssl-dev \\\n  libicu-dev \\\n  libsodium-dev \\\n  libssl-dev \\\n  libx11-dev \\\n  make \\\n  zlib1g-dev \\\n  && apt-get clean\n\nCOPY vetiver_renv.lock renv.lock\nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\nCOPY plumber.R /opt/ml/plumber.R\nEXPOSE 8000\nENTRYPOINT [\"R\", \"-e\", \"pr &lt;- plumber::plumb('/opt/ml/plumber.R'); pr$run(host = '0.0.0.0', port = 8000)\"]\n\n\nWhen you run vetiver_prepare_docker(), you generate three files needed to build a Docker image: the Dockerfile itself, a Plumber file serving your REST API, and the vetiver_renv.lock file to capture your model dependencies.\n\n\n\nvetiver.prepare_docker(model_board, \"cars_mpg\")\n\n\n\n/opt/hostedtoolcache/Python/3.11.12/x64/lib/python3.11/site-packages/vetiver/attach_pkgs.py:77: UserWarning:\n\nrequired packages unknown for board protocol: ('file', 'local'), add to model's metadata to export\n\n\n# # Generated by the vetiver package; edit with care\n# start with python base image\nFROM python:3.11\n\n# create directory in container for vetiver files\nWORKDIR /vetiver\n\n# copy  and install requirements\nCOPY vetiver_requirements.txt /vetiver/requirements.txt\n\n#\nRUN pip install --no-cache-dir --upgrade -r /vetiver/requirements.txt\n\n# copy app file\nCOPY app.py /vetiver/app/app.py\n\n# expose port\nEXPOSE 8080\n\n# run vetiver API\nCMD [\"uvicorn\", \"app.app:api\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n\n\nWhen you run vetiver.prepare_docker(), you generate three files needed to build a Docker image: the Dockerfile itself, a FastAPI app file serving your REST API, and a requirements.txt file to capture your model dependencies.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nWhen you build such a Docker container with docker build, all the packages needed to make a prediction with your model are installed into the container.\nWhen you run the Docker container, you can pass in environment variables (for authentication to your pins board, for example) with docker run --env-file .Renviron.\nLearn more about deploying with Docker.",
    "crumbs": [
      "Get Started",
      "Deploy"
    ]
  },
  {
    "objectID": "get-started/deploy.html#predict-from-your-model-endpoint",
    "href": "get-started/deploy.html#predict-from-your-model-endpoint",
    "title": "Deploy",
    "section": "Predict from your model endpoint",
    "text": "Predict from your model endpoint\nA model deployed via vetiver can be treated as a special vetiver_endpoint() object.\n\nRPython\n\n\n\nendpoint &lt;- vetiver_endpoint(\"http://127.0.0.1:8080/predict\")\nendpoint\n\n\n── A model API endpoint for prediction: \nhttp://127.0.0.1:8080/predict\n\n\n\n\n\nfrom vetiver.server import predict, vetiver_endpoint\nendpoint = vetiver_endpoint(\"http://127.0.0.1:8080/predict\")\nendpoint\n\n'http://127.0.0.1:8080/predict'\n\n\n\n\n\nIf such a deployed model endpoint is running via one process (either remotely on a server or locally, perhaps via Docker or a background job in the RStudio IDE), you can make predictions with that deployed model and new data in another, separate process1.\n\nRPython\n\n\n\nnew_car &lt;- tibble(cyl = 4,  disp = 200, \n                  hp = 100, drat = 3,\n                  wt = 3,   qsec = 17, \n                  vs = 0,   am = 1,\n                  gear = 4, carb = 2)\npredict(endpoint, new_car)\n\n# A tibble: 11 × 1\n   .pred\n   &lt;chr&gt;      \n 1 22.3       \n\n\n\nimport pandas as pd\nnew_car_dict = {\"cyl\": [4], \"disp\": [200], \n                 \"hp\": [100], \"drat\": [3],\n                 \"wt\": [3], \"qsec\": [17], \n                 \"vs\": [0], \"am\": [1],\n                 \"gear\": [4], \"carb\": [2]}\nnew_car = pd.DataFrame(new_car_dict)\npredict(endpoint, new_car)\n\n  prediction\n0       21.0\n\n\n\nBeing able to predict with a vetiver model endpoint takes advantage of the model’s input data prototype and other metadata that is stored with the model.",
    "crumbs": [
      "Get Started",
      "Deploy"
    ]
  },
  {
    "objectID": "get-started/deploy.html#footnotes",
    "href": "get-started/deploy.html#footnotes",
    "title": "Deploy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKeep in mind that the R and Python models have different values for the decision tree hyperparameters.↩︎",
    "crumbs": [
      "Get Started",
      "Deploy"
    ]
  }
]