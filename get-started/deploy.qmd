---
title: "Deploy"
format:
  html:
    toc: true
    include-after-body:
      - sync-tabs.html
---

## Review of previous steps

::: panel-tabset
## R

```{r}
#| code-fold: true
#| code-summary: "Show the code from previous steps"
#| message: false
library(tidymodels)
library(vetiver)
library(pins)

car_mod <-
    workflow(mpg ~ ., decision_tree(mode = "regression")) %>%
    fit(mtcars)
v <- vetiver_model(car_mod, "cars_mpg")
model_board <- board_folder(".", versioned = TRUE)
model_board %>% vetiver_pin_write(v)
```

## Python

```{python}
#| code-fold: true
#| code-summary: "Show the code from previous steps"
#| message: false

from vetiver.data import mtcars
from vetiver import VetiverModel, vetiver_pin_write
from sklearn import tree
from pins import board_folder

car_mod = tree.DecisionTreeRegressor().fit(mtcars, mtcars["mpg"])

v = VetiverModel(car_mod, model_name = "cars_mpg", 
                 save_ptype = True, ptype_data = mtcars)

model_board = board_folder(".", allow_pickle_read=True)
vetiver_pin_write(model_board, v)
```
:::

## Deploy your model

You can deploy your model by creating a special [Plumber](https://www.rplumber.io/) router in R or a [FastAPI](https://fastapi.tiangolo.com/) router in Python, and adding a POST endpoint for making predictions.

::: panel-tabset
## R

```{r}
library(plumber)
pr() %>%
  vetiver_api(v)
```

To start a server using this object, pipe (`%>%`) to `pr_run(port = 8080)` or your port of choice.

## Python

```{python}
from vetiver import VetiverAPI
app = VetiverAPI(v, check_ptype = True)
```

To start a server using this object, use `app.run(port = 8080)` or your port of choice.
:::

You can interact with your vetiver API locally and debug it. FastAPI and Plumber APIs such as these can be hosted in a variety of ways. You can create a ready-to-go file for deployment that is especially suited for [RStudio Connect](https://www.rstudio.com/products/connect/).

::: panel-tabset
## R

```{r}
#| eval: false
vetiver_write_plumber(model_board, "cars_mpg")
```

```{r}
#| echo: false
#| comment: ""
docker_dir <- fs::path_real(tempdir())
tmp_plumber <- fs::path(docker_dir, "plumber.R")
vetiver_write_plumber(model_board, "cars_mpg", file = tmp_plumber)
cat(readr::read_lines(tmp_plumber), sep = "\n")
```

For RStudio Connect, you can streamline this deployment process even more by using `vetiver_deploy_rsconnect(model_board, "cars_mpg")`.

## Python

```{python}
#| eval: false
app_file = vetiver_write_app(model_board, "cars_mpg")
```

```{python}
#| echo: false
#| comment: ""
from vetiver import vetiver_write_app
import tempfile
with tempfile.TemporaryDirectory() as temp:
  tmp = temp + "app.py"
  vetiver_write_app(model_board, "cars_mpg", file=tmp)
  contents = open(tmp).read()
print(contents.replace(temp, "."))
```
:::

In a real-world situation, you would see something like `board_rsconnect()` or `board_s3()` here instead of our temporary demo board.

::: callout-important
Notice that the deployment is strongly linked to a specific version of the pinned model; if you pin another version of the model after you deploy your model, your deployed model will not be affected.
:::

## Generate a Dockerfile

For deploying a vetiver API to infrastructure other than RStudio Connect, such as [Google Cloud Run](https://cloud.google.com/run/docs/deploying), [AWS](https://docs.aws.amazon.com/AmazonECS/latest/userguide/create-container-image.html), or [Azure](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quickstart), you likely will want to build a Docker container.

::: callout-note
You can use any pins board with Docker, like `board_folder()` or `board_rsconnect()`, as long as your Docker container can authenticate to your pins board.
:::

::: panel-tabset
## R

```{r}
#| eval: false
vetiver_write_docker(v)
```

```{r}
#| echo: false
#| message: false
#| comment: ""
options(renv.verbose = FALSE)
vetiver_write_docker(v, tmp_plumber, docker_dir)
docker_contents <- readr::read_lines(fs::path(docker_dir, "Dockerfile"))
rel_dir <- fs::path_rel(docker_dir)
docker_contents <- gsub(paste0(rel_dir, "/"), "", docker_contents, fixed = TRUE)
docker_contents <- gsub(paste0(docker_dir, "/"), "", docker_contents, fixed = TRUE)
cat(docker_contents, sep = "\n")
```

When you run `vetiver_write_docker()`, you generate *two* files: the Dockerfile itself and [the `renv.lock` file](https://rstudio.github.io/renv/articles/lockfile.html) to capture your model dependencies.

## Python

```{python}
#| eval: false
vetiver_write_docker(app_file)
```

```{python}
#| echo: false
#| comment: ""
from vetiver import vetiver_write_docker
import tempfile

with tempfile.TemporaryDirectory() as temp:
  tmp_app = temp + "/app.py"
  tmp_docker = temp + "/Dockerfile"
  vetiver_write_app(model_board, "cars_mpg", file=tmp_app)
  vetiver_write_docker(app_file=tmp_app, path=temp+"/")
  contents = open(tmp_docker).read()
print(contents.replace(temp, "."))

```

To build the Docker image, you need *two* files: the Dockerfile itself generated via `vetiver_write_docker()` and [a `requirements.txt` file](https://pip.pypa.io/en/stable/reference/requirements-file-format/) to capture your model dependencies. If you don't already have a requirements file for your project, `vetiver.load_pkgs()` will generate one for you, with the name `vetiver_requirements.txt`.
:::

::: callout-tip
-   When you build such a Docker container [with `docker build`](https://docs.docker.com/engine/reference/commandline/build/), all the packages needed to make a prediction with your model are installed into the container.

-   When you run the Docker container, you can pass in environment variables (for authentication to your pins board, for example) with `docker run --env-file .Renviron`.
:::

## Predict from your model endpoint

A model deployed via vetiver can be treated as a special `vetiver_endpoint()` object.

::: panel-tabset
## R

```{r}
endpoint <- vetiver_endpoint("http://127.0.0.1:8080/predict")
endpoint
```

## Python

```{python}
from vetiver.server import predict, vetiver_endpoint
endpoint = vetiver_endpoint("http://127.0.0.1:8080/predict")
endpoint
```
:::

If such a deployed model endpoint is running via one process (either remotely on a server or locally, perhaps via [a background job in the RStudio IDE](https://solutions.rstudio.com/r/jobs/)), you can make predictions with that deployed model and new data in another, separate process[^1].

[^1]: Keep in mind that the R and Python models have different values for the decision tree hyperparameters.

::: panel-tabset
## R

```{r}
#| eval: false
new_car <- tibble(cyl = 4,  disp = 200, 
                  hp = 100, drat = 3,
                  wt = 3,   qsec = 17, 
                  vs = 0,   am = 1,
                  gear = 4, carb = 2)
predict(endpoint, new_car)
```

    # A tibble: 11 Ã— 1
       .pred
       <chr>      
     1 22.3       

## Python

```{python}
#| eval: false
import pandas as pd
new_car_dict = {"cyl": [4], "disp": [200], 
                 "hp": [100], "drat": [3],
                 "wt": [3], "qsec": [17], 
                 "vs": [0], "am": [1],
                 "gear": [4], "carb": [2]}
new_car = pd.DataFrame(new_car_dict)
predict(endpoint, new_car)
```

      prediction
    0       22.3
:::

Being able to predict with a vetiver model endpoint takes advantage of the model's input data prototype and other metadata that is stored with the model.
