---
title: "Deploy"
format:
  html:
    toc: true
---

## Review of previous steps

::: {.panel-tabset group="language"}
## R

```{r}
#| code-fold: true
#| code-summary: "Show the code from previous steps"
#| message: false
library(tidymodels)
library(vetiver)
library(pins)

car_mod <-
    workflow(mpg ~ ., decision_tree(mode = "regression")) %>%
    fit(mtcars)
v <- vetiver_model(car_mod, "cars_mpg")
model_board <- board_folder("pins-r", versioned = TRUE)
model_board %>% vetiver_pin_write(v)
```

## Python

```{python}
#| code-fold: true
#| code-summary: "Show the code from previous steps"
#| output: false

from vetiver.data import mtcars
from vetiver import VetiverModel, vetiver_pin_write
from sklearn import tree
from pins import board_folder

car_mod = tree.DecisionTreeRegressor().fit(mtcars.drop(columns="mpg"), mtcars["mpg"])

v = VetiverModel(car_mod, model_name = "cars_mpg", 
                 prototype_data = mtcars.drop(columns="mpg"))

model_board = board_folder("pins-py", allow_pickle_read=True)
vetiver_pin_write(model_board, v)
```
:::

## Create a REST API for deployment

You can deploy your model by creating a special [Plumber](https://www.rplumber.io/) router in R or a [FastAPI](https://fastapi.tiangolo.com/) router in Python, and adding a POST endpoint for making predictions.

::: {.panel-tabset group="language"}
## R

```{r}
library(plumber)
pr() %>%
  vetiver_api(v)
```

To start a server using this object, pipe (`%>%`) to `pr_run(port = 8080)` or your port of choice.

## Python

```{python}
from vetiver import VetiverAPI
app = VetiverAPI(v, check_ptype = True)
```

To start a server using this object, use `app.run(port = 8080)` or your port of choice.
:::

You can interact with your vetiver API via automatically generated, detailed visual documentation.

<script src="https://fast.wistia.com/embed/medias/w7p0op712v.jsonp" async></script><script src="https://fast.wistia.com/assets/external/E-v1.js" async></script><div class="wistia_responsive_padding" style="padding:107.92% 0 0 0;position:relative;"><div class="wistia_responsive_wrapper" style="height:100%;left:0;position:absolute;top:0;width:100%;"><div class="wistia_embed wistia_async_w7p0op712v videoFoam=true" style="height:100%;position:relative;width:100%"><div class="wistia_swatch" style="height:100%;left:0;opacity:0;overflow:hidden;position:absolute;top:0;transition:opacity 200ms;width:100%;"><img src="https://fast.wistia.com/embed/medias/w7p0op712v/swatch" style="filter:blur(5px);height:100%;object-fit:contain;width:100%;" alt="" aria-hidden="true" onload="this.parentNode.style.opacity=1;" /></div></div></div></div>
<br>

FastAPI and Plumber APIs such as these can be hosted in a variety of ways. Let's walk through two options: deploying to Posit Connect or with Docker.

## Deploy to Connect

For [Posit Connect](https://posit.co/products/enterprise/connect/), you can deploy your versioned model with a single function.


::: {.panel-tabset group="language"}
## R

```{r}
#| eval: false
# authenticates via environment variables:
vetiver_deploy_rsconnect(model_board, "user.name/cars_mpg")
```

## Python

```{python}
#| eval: false
from rsconnect.api import RSConnectServer

connect_server = RSConnectServer(
    url=server_url, # load from an .env file
    api_key=api_key # load from an .env file 
)

vetiver.deploy_rsconnect(
    connect_server = connect_server,
    board = model_board,
    pin_name = "user.name/cars_mpg",
)
```

:::

In this case, you probably want `model_board` to be a Connect pins board (`board_connect()`). For more on deploying to Connect, see the [Connect documentation for using vetiver](https://docs.posit.co/connect/user/vetiver/).

## Prepare a Dockerfile

For deploying a vetiver API to infrastructure other than Posit Connect, such as [Google Cloud Run](https://cloud.google.com/run/docs/deploying), [AWS](https://docs.aws.amazon.com/AmazonECS/latest/userguide/create-container-image.html), or [Azure](https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quickstart), you likely will want to build a Docker container.

::: callout-note
You can use any pins board with Docker, like `board_folder()` or `board_connect()`, as long as your Docker container can authenticate to your pins board.
:::

::: {.panel-tabset group="language"}
## R

```{r}
#| eval: false
vetiver_prepare_docker(model_board, "cars_mpg")
```

```{r}
#| echo: false
#| message: false
#| comment: ""
options(renv.verbose = FALSE)
docker_dir <- fs::path_real(tempdir())
tmp_plumber <- fs::path(docker_dir, "plumber.R")
vetiver_write_plumber(model_board, "cars_mpg", file = tmp_plumber, rsconnect = FALSE)
vetiver_write_docker(v, tmp_plumber, docker_dir)
docker_contents <- readr::read_lines(fs::path(docker_dir, "Dockerfile"))
rel_dir <- fs::path_rel(docker_dir)
docker_contents <- gsub(paste0(rel_dir, "/"), "", docker_contents, fixed = TRUE)
docker_contents <- gsub(paste0(docker_dir, "/"), "", docker_contents, fixed = TRUE)
cat(docker_contents, sep = "\n")
```

When you run `vetiver_prepare_docker()`, you generate *three* files needed to build a Docker image: the Dockerfile itself, a Plumber file serving your REST API, and [the `vetiver_renv.lock` file](https://rstudio.github.io/renv/articles/lockfile.html) to capture your model dependencies.

## Python

```{python}
#| eval: false
vetiver.prepare_docker(model_board, "cars_mpg")
```

```{python}
#| echo: false
#| comment: ""
from vetiver import write_docker
import tempfile

with tempfile.TemporaryDirectory() as temp:
  tmp_app = temp + "/app.py"
  tmp_docker = temp + "/Dockerfile"
  write_app(model_board, "cars_mpg", file=tmp_app)
  write_docker(app_file=tmp_app, path=temp+"/")
  contents = open(tmp_docker).read()
print(contents.replace(temp, "."))

```

When you run `vetiver.prepare_docker()`, you generate *three* files needed to build a Docker image: the Dockerfile itself, a FastAPI app file serving your REST API, and [a `requirements.txt` file](https://pip.pypa.io/en/stable/reference/requirements-file-format/) to capture your model dependencies.
:::

::: callout-tip
-   When you build such a Docker container [with `docker build`](https://docs.docker.com/engine/reference/commandline/build/), all the packages needed to make a prediction with your model are installed into the container.

-   When you run the Docker container, you can pass in environment variables (for authentication to your pins board, for example) with `docker run --env-file .Renviron`.

-   Learn more about [deploying with Docker](https://vetiver.rstudio.com/learn-more/deploy-with-docker.html).
:::

## Predict from your model endpoint

A model deployed via vetiver can be treated as a special `vetiver_endpoint()` object.

::: {.panel-tabset group="language"}
## R

```{r}
endpoint <- vetiver_endpoint("http://127.0.0.1:8080/predict")
endpoint
```

## Python

```{python}
from vetiver.server import predict, vetiver_endpoint
endpoint = vetiver_endpoint("http://127.0.0.1:8080/predict")
endpoint
```
:::

If such a deployed model endpoint is running via one process (either remotely on a server or locally, perhaps via Docker or [a background job in the RStudio IDE](https://docs.posit.co/ide/user/ide/guide/tools/jobs.html)), you can make predictions with that deployed model and new data in another, separate process[^1].

[^1]: Keep in mind that the R and Python models have different values for the decision tree hyperparameters.

::: {.panel-tabset group="language"}
## R

```{r}
#| eval: false
new_car <- tibble(cyl = 4,  disp = 200, 
                  hp = 100, drat = 3,
                  wt = 3,   qsec = 17, 
                  vs = 0,   am = 1,
                  gear = 4, carb = 2)
predict(endpoint, new_car)
```

    # A tibble: 11 Ã— 1
       .pred
       <chr>      
     1 22.3       

## Python

```{python}
#| eval: false
import pandas as pd
new_car_dict = {"cyl": [4], "disp": [200], 
                 "hp": [100], "drat": [3],
                 "wt": [3], "qsec": [17], 
                 "vs": [0], "am": [1],
                 "gear": [4], "carb": [2]}
new_car = pd.DataFrame(new_car_dict)
predict(endpoint, new_car)
```

      prediction
    0       21.0
:::

Being able to predict with a vetiver model endpoint takes advantage of the model's input data prototype and other metadata that is stored with the model.
